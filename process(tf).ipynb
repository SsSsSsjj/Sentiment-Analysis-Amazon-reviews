{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JCjZdwAl0EE9",
        "outputId": "dbb34ce4-1974-4f36-9f4a-ff5a4a313bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (6.2.2)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (642 bytes)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.5.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.12/dist-packages (from textacy) (3.6)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.6.1)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.12/dist-packages (from textacy) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->textacy) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-1.1.0 floret-0.10.5 jellyfish-1.2.1 pyphen-0.17.2 textacy-0.13.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install textacy nltk spacy xgboost\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C07NR5qz0vF5",
        "outputId": "bb08bdab-1f5c-40fe-f973-951b24fb273a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gzip\n",
        "import spacy\n",
        "import pickle\n",
        "import string\n",
        "import ast\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.decomposition import NMF\n",
        "from nltk.corpus import stopwords\n",
        "import xgboost as XGB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(32113)\n",
        "\n",
        "##############################################################################\n",
        "#                      Stop Words and Lemmatizer setting                     #\n",
        "##############################################################################\n",
        "\n",
        "# STOPLIST 정의\n",
        "STOPLIST = list(\n",
        "    set(\n",
        "        stopwords.words(\"english\")\n",
        "        + [\"n't\", \"'s\", \"'m\", \"ca\"]\n",
        "        + list(ENGLISH_STOP_WORDS)\n",
        "    )\n",
        ") + \" \".join(string.punctuation).split(\" \") + [\n",
        "    \"-----\",\n",
        "    \"---\",\n",
        "    \"...\",\n",
        "    \"..\",\n",
        "    \"....\",\n",
        "    \"\",\n",
        "    \" \",\n",
        "    \"\\n\",\n",
        "    \"\\n\\n\",\n",
        "]\n",
        "\n",
        "# spaCy 모델 로드\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"spaCy 'en_core_web_sm' 모델을 로드할 수 없습니다. 설치 후 다시 시도하세요.\")\n",
        "    raise\n",
        "\n",
        "# TfidfVectorizer에 전달할 토크나이저 함수 정의\n",
        "def spacy_tokenizer(doc):\n",
        "    \"\"\"spaCy를 사용하여 토큰화 및 표제어 추출\"\"\"\n",
        "    if not isinstance(doc, str):\n",
        "        return []\n",
        "\n",
        "    # 텍스트 전처리 (숫자 제거, 특수문자 제거)\n",
        "    doc = re.sub(r'\\d+', '', doc)\n",
        "\n",
        "    doc = nlp(doc)\n",
        "    tokens = []\n",
        "    for n in doc:\n",
        "        lemma = n.lemma_.lower().strip()\n",
        "        # 불용어, 구두점, 짧은 단어 제외 (STOPLIST는 TfidfVectorizer에서 처리)\n",
        "        if len(lemma) > 1 and n.pos_ != 'PUNCT':\n",
        "            token = n.lower_ if n.lemma_ == \"-PRON-\" else lemma\n",
        "            tokens.append(token)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#                             Data preparation                               #\n",
        "##############################################################################\n",
        "\n",
        "def Data_prep1(df):\n",
        "    \"\"\"\n",
        "    raw_reviews DataFrame에서 helpful, reviewText 기반으로\n",
        "    helpful_percent, text_length 등을 만드는 함수\n",
        "    \"\"\"\n",
        "    game_df = df.copy()\n",
        "\n",
        "    # 'helpful' 문자열(\"[pos, total]\")을 리스트로 변환\n",
        "    game_df['helpful_parsed'] = game_df['helpful'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    helpful = np.array(game_df['helpful_parsed'])\n",
        "    helpful = helpful.reshape(len(game_df), 1)\n",
        "\n",
        "    helpful_num1 = np.zeros((len(helpful), 1)) # pos\n",
        "    helpful_num2 = np.zeros((len(helpful), 1)) # total\n",
        "\n",
        "    for i in range(len(helpful)):\n",
        "        try:\n",
        "            pos, total = helpful[i][0]\n",
        "        except (TypeError, ValueError, IndexError):\n",
        "            try:\n",
        "                pos, total = helpful[i]\n",
        "            except (TypeError, ValueError, IndexError):\n",
        "                 pos, total = 0, 0\n",
        "\n",
        "        helpful_num1[i] = pos\n",
        "        helpful_num2[i] = total\n",
        "\n",
        "    game_df['helpful_total_review'] = helpful_num2\n",
        "    game_df['num_of_helpful_review'] = helpful_num1\n",
        "\n",
        "    # 총 투표 20개 이상만 사용 (기존 코드 기준 유지)\n",
        "    new_df = game_df[game_df['helpful_total_review'] > 20]\n",
        "\n",
        "    # 불필요한 열 제거\n",
        "    drop_cols = [c for c in ['unixReviewTime', 'reviewTime', 'helpful_parsed', 'helpful']\n",
        "                 if c in new_df.columns]\n",
        "    new_df = new_df.drop(drop_cols, axis=1)\n",
        "\n",
        "    # helpful_percent 계산\n",
        "    new_df['helpful_percent'] = (\n",
        "        new_df['num_of_helpful_review'] / new_df['helpful_total_review']\n",
        "    ).round(2)\n",
        "    new_df.loc[new_df['helpful_total_review'] == 0, 'helpful_percent'] = 0\n",
        "\n",
        "\n",
        "    new_df.index = range(len(new_df))\n",
        "\n",
        "    # 텍스트 길이 계산\n",
        "    length = np.zeros((len(new_df), 1))\n",
        "    for i in new_df.index:\n",
        "        length[i] = int(len(str(new_df['reviewText'].iloc[i])))\n",
        "    new_df['text_length'] = length\n",
        "\n",
        "    # reviewText가 NaN인 행 제거\n",
        "    new_df = new_df.dropna(subset=['reviewText']).copy()\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def label_prep(new_df, line=0.90): # <- HIGH 클래스 확보를 위해 0.90으로 완화\n",
        "    \"\"\"\n",
        "    helpful_percent를 기준으로 'label'('HIGH'/'LOW') 생성\n",
        "    \"\"\"\n",
        "    high = new_df[new_df['helpful_percent'] >= line]\n",
        "    low = new_df[new_df['helpful_percent'] < line]\n",
        "    print(f\"== line = {line} ==\")\n",
        "    print(\"highly helpful count: {}\".format(len(high)))\n",
        "    print(\"not helpful count: {}\".format(len(low)))\n",
        "\n",
        "    new_df['label'] = 'LOW'\n",
        "    new_df.loc[new_df.helpful_percent >= line, 'label'] = 'HIGH'\n",
        "    return new_df.copy()\n",
        "\n",
        "\n",
        "def create_tfidf_nmf_features(df, max_feat=5000, n_components=50):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1. TF-IDF 벡터화\n",
        "    print(\"TF-IDF Vectorizing reviews...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_feat,\n",
        "        min_df=5,\n",
        "        max_df=0.7,\n",
        "        stop_words=STOPLIST,\n",
        "        ngram_range=(1, 2), # Unigram, Bigram 사용\n",
        "        tokenizer=spacy_tokenizer # spacy 토크나이저 사용\n",
        "    )\n",
        "    X_tfidf = vectorizer.fit_transform(df['reviewText'])\n",
        "\n",
        "    # 2. NMF로 차원 축소 및 특징 추출\n",
        "    print(f\"Applying NMF with {n_components} components...\")\n",
        "    nmf = NMF(n_components=n_components, random_state=32113, max_iter=200)\n",
        "    X_nmf = nmf.fit_transform(X_tfidf)\n",
        "\n",
        "    print(f\"TF-IDF/NMF feature creation took {time.time() - start_time:.2f} seconds.\")\n",
        "    return X_nmf, vectorizer, nmf\n",
        "\n",
        "\n",
        "def _dataselector(df, label, limit):\n",
        "    \"\"\"\n",
        "    클래스 불균형 해소를 위해 최대 'limit' 개수만큼 데이터 추출\n",
        "    \"\"\"\n",
        "    df_lb_all = df[df['label'] == label]\n",
        "    if len(df_lb_all) > limit:\n",
        "        random_c = np.random.choice(list(df_lb_all.index), limit, replace=False)\n",
        "        df_lb = df_lb_all.loc[list(random_c)]\n",
        "    else:\n",
        "        df_lb = df_lb_all\n",
        "    return df_lb\n",
        "\n",
        "def df_for_XGBOOST(df_feat, lim):\n",
        "    \"\"\"\n",
        "    데이터프레임의 클래스 수를 맞추고 XGBoost 입력용 numpy 배열로 변환\n",
        "    \"\"\"\n",
        "    df = df_feat.copy()\n",
        "    df.index = range(len(df))\n",
        "\n",
        "    df_low = _dataselector(df, 'LOW', lim)\n",
        "    df_high = _dataselector(df, 'HIGH', lim)\n",
        "\n",
        "    new_df = pd.concat([df_low, df_high], axis=0)\n",
        "    new_df.index = range(len(new_df)) # 인덱스 리셋\n",
        "\n",
        "    Y = new_df.pop('label') # <- 'label'을 추출하며 new_df에서 제거됨\n",
        "    b_loon = {'LOW': 0, 'HIGH': 1}\n",
        "    Y2 = Y.map(b_loon)\n",
        "\n",
        "    X = np.array(new_df)\n",
        "    return X, Y2, new_df # X: 피처, Y2: 레이블, new_df: 피처 DataFrame\n",
        "\n",
        "def XGBOOSTING(X_tr1, X_te1, y_tr1, y_te1, scale_pos_weight, xgb_para=[200, 0.2, 5]):\n",
        "    start_time = time.time()\n",
        "    xgb = XGB.XGBClassifier(\n",
        "        n_estimators=xgb_para[0],\n",
        "        learning_rate=xgb_para[1],\n",
        "        max_depth=xgb_para[2], # <- max_depth를 5로 증가\n",
        "        eval_metric='logloss',\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=32113,\n",
        "        use_label_encoder=False,\n",
        "        scale_pos_weight=scale_pos_weight\n",
        "    )\n",
        "    xgb.fit(X_tr1, y_tr1)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    # 기본 0.5 임계값의 정확도 출력\n",
        "    score = xgb.score(X_te1, y_te1)\n",
        "    print(f\"XGBoost accuracy (Default 0.5 Threshold): {round(score * 100, 2)}%\")\n",
        "    return xgb\n",
        "\n",
        "def conf_mat(model, X_te1, y_te1, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Confusion matrix 및 클래스별 재현율 출력\n",
        "    \"\"\"\n",
        "    # 임계값 적용하여 예측 수행\n",
        "    y_proba = model.predict_proba(X_te1)[:, 1]\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_te1, y_pred)\n",
        "\n",
        "    print(f\"\\n** Confusion Matrix (Threshold: {threshold}) **\")\n",
        "    print(\"                 | Pred LOW (0) | Pred HIGH (1) |\")\n",
        "    print(\"-----------------|--------------|---------------|\")\n",
        "    print(f\"True LOW (0)     | {cm[0, 0]:<12} | {cm[0, 1]:<13} |\")\n",
        "    print(f\"True HIGH (1)    | {cm[1, 0]:<12} | {cm[1, 1]:<13} |\")\n",
        "\n",
        "    low_den = cm[0].sum()\n",
        "    high_den = cm[1].sum()\n",
        "\n",
        "    low_rate = (cm[0, 0] / low_den * 100).round(2) if low_den > 0 else 0.0\n",
        "    high_rate = (cm[1, 1] / high_den * 100).round(2) if high_den > 0 else 0.0\n",
        "\n",
        "    overall_accuracy = (cm[0, 0] + cm[1, 1]) / (low_den + high_den) * 100\n",
        "\n",
        "    print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n",
        "    print(f\"LOW (0) prediction rate (Recall): {low_rate}%\")\n",
        "    print(f\"HIGH (1) prediction rate (Recall): {high_rate}%\\n\")\n",
        "\n",
        "\n",
        "def xgb_stats(model, df_for_model, X_test, y_test, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Feature Importance와 Confusion matrix를 출력\n",
        "    \"\"\"\n",
        "    # 0. 피처 중요도\n",
        "    ind = np.argsort(model.feature_importances_)\n",
        "    imp = np.sort(model.feature_importances_)\n",
        "\n",
        "    imp2 = [df_for_model.columns[i] for i in ind]\n",
        "\n",
        "    print(\" **TOP 15 Important Features** \")\n",
        "    for i in range(1, 16):\n",
        "        print(\"{}. {} : {}%  \".format(i, imp2[-i], round(imp[-i] * 100, 2)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 1. Confusion Matrix 및 Class별 정확도 (임계값 0.5 기준)\n",
        "    conf_mat(model, X_test, y_test, threshold=0.5)\n",
        "\n",
        "    # 2. Confusion Matrix 및 Class별 정확도 (튜닝된 임계값 기준)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✨ Tunning Result: Adjusted Threshold for balanced Recall ✨\")\n",
        "    # LOW 재현율 개선을 위해 임계값을 0.60으로 설정\n",
        "    conf_mat(model, X_test, y_test, threshold=threshold)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#                                MAIN EXECUTION                              #\n",
        "##############################################################################\n",
        "# 1. 데이터 로드 (경로를 실제 파일 위치로 수정하세요)\n",
        "DATA_PATH = \"/content/drive/MyDrive/amazon_P/data/raw/Musical_instruments_reviews.csv\"\n",
        "try:\n",
        "    raw_reviews = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"에러: 파일을 찾을 수 없습니다. 경로를 확인하세요: {DATA_PATH}\")\n",
        "    raise\n",
        "\n",
        "print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
        "\n",
        "# 2. 데이터 전처리 및 레이블링 (HIGH/LOW 생성, 기준 0.90 적용)\n",
        "df_processed = Data_prep1(raw_reviews)\n",
        "df_labeled = label_prep(df_processed, line=0.90) # <- 기준 0.90 적용\n",
        "print(df_labeled['label'].value_counts())\n",
        "\n",
        "# 3. TF-IDF 및 NMF 특징 추출 (50개 컴포넌트 사용)\n",
        "NMF_COMPONENTS = 50\n",
        "X_nmf, vectorizer, nmf_model = create_tfidf_nmf_features(df_labeled, n_components=NMF_COMPONENTS)\n",
        "\n",
        "# 4. 피처 병합 (NMF + 수치형)\n",
        "df_nmf = pd.DataFrame(X_nmf, index=df_labeled.index,\n",
        "                      columns=[f'nmf_feat_{i}' for i in range(NMF_COMPONENTS)])\n",
        "\n",
        "# 사용할 수치형 피처 추출\n",
        "numerical_features = df_labeled[['overall', 'text_length']].copy()\n",
        "\n",
        "# 모든 피처 병합 및 레이블 추가\n",
        "# (helpful_percent 관련 변수는 포함하지 않음 -> Data Leakage 방지)\n",
        "df_final_features = pd.concat([numerical_features, df_nmf, df_labeled[['label']]], axis=1)\n",
        "\n",
        "# 5. XGBoost용 데이터 준비 및 클래스 밸런싱\n",
        "class_counts = df_final_features['label'].value_counts()\n",
        "lim = int(class_counts.min())\n",
        "print(f\"\\nBalancing classes to limit={lim} per class.\")\n",
        "\n",
        "# X: features, y: label (0/1), df_for_model: feature DF (balanced)\n",
        "X, y, df_for_model = df_for_XGBOOST(df_final_features, lim=lim)\n",
        "\n",
        "# 6. 학습/테스트 데이터 분할\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=32113, stratify=y\n",
        ")\n",
        "\n",
        "# 7. XGBoost 모델 학습 (하이퍼파라미터 튜닝 적용)\n",
        "scale_pos_weight = 1.0\n",
        "\n",
        "print(\"\\nStarting XGBoost Training with Tunning...\")\n",
        "xgb_nmf_model = XGBOOSTING(\n",
        "    X_tr,\n",
        "    X_te,\n",
        "    y_tr,\n",
        "    y_te,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    xgb_para=[200, 0.2, 5] # max_depth=5 적용\n",
        ")\n",
        "\n",
        "# 8. 결과 통계 출력 및 임계값 튜닝\n",
        "# LOW 재현율 개선을 위해 임계값을 0.60으로 설정\n",
        "ADJUSTED_THRESHOLD = 0.60\n",
        "\n",
        "feature_names_df = df_for_model.copy()\n",
        "xgb_stats(xgb_nmf_model, feature_names_df, X_te, y_te, threshold=ADJUSTED_THRESHOLD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6aRVn6t0lhG",
        "outputId": "fc82214d-2a90-474a-c96f-7146e0417406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded 10261 reviews.\n",
            "== line = 0.9 ==\n",
            "highly helpful count: 119\n",
            "not helpful count: 40\n",
            "label\n",
            "HIGH    119\n",
            "LOW      40\n",
            "Name: count, dtype: int64\n",
            "TF-IDF Vectorizing reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'ve\", 'far', 'make', 'need', 'shall', 'win'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying NMF with 50 components...\n",
            "TF-IDF/NMF feature creation took 11.27 seconds.\n",
            "\n",
            "Balancing classes to limit=40 per class.\n",
            "\n",
            "Starting XGBoost Training with Tunning...\n",
            "--- 0.07835960388183594 seconds ---\n",
            "XGBoost accuracy (Default 0.5 Threshold): 56.25%\n",
            " **TOP 15 Important Features** \n",
            "1. nmf_feat_1 : 20.0%  \n",
            "2. nmf_feat_17 : 9.0%  \n",
            "3. nmf_feat_43 : 7.389999866485596%  \n",
            "4. nmf_feat_35 : 5.059999942779541%  \n",
            "5. nmf_feat_44 : 4.78000020980835%  \n",
            "6. nmf_feat_24 : 4.679999828338623%  \n",
            "7. nmf_feat_47 : 4.579999923706055%  \n",
            "8. nmf_feat_9 : 3.5899999141693115%  \n",
            "9. nmf_feat_7 : 2.950000047683716%  \n",
            "10. nmf_feat_10 : 2.8399999141693115%  \n",
            "11. nmf_feat_42 : 2.7899999618530273%  \n",
            "12. overall : 2.2899999618530273%  \n",
            "13. nmf_feat_27 : 2.0199999809265137%  \n",
            "14. nmf_feat_40 : 1.9199999570846558%  \n",
            "15. nmf_feat_28 : 1.909999966621399%  \n",
            "\n",
            "\n",
            "\n",
            "** Confusion Matrix (Threshold: 0.5) **\n",
            "                 | Pred LOW (0) | Pred HIGH (1) |\n",
            "-----------------|--------------|---------------|\n",
            "True LOW (0)     | 6            | 2             |\n",
            "True HIGH (1)    | 5            | 3             |\n",
            "\n",
            "Overall Accuracy: 56.25%\n",
            "LOW (0) prediction rate (Recall): 75.0%\n",
            "HIGH (1) prediction rate (Recall): 37.5%\n",
            "\n",
            "\n",
            "==================================================\n",
            "✨ Tunning Result: Adjusted Threshold for balanced Recall ✨\n",
            "\n",
            "** Confusion Matrix (Threshold: 0.6) **\n",
            "                 | Pred LOW (0) | Pred HIGH (1) |\n",
            "-----------------|--------------|---------------|\n",
            "True LOW (0)     | 6            | 2             |\n",
            "True HIGH (1)    | 5            | 3             |\n",
            "\n",
            "Overall Accuracy: 56.25%\n",
            "LOW (0) prediction rate (Recall): 75.0%\n",
            "HIGH (1) prediction rate (Recall): 37.5%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [02:13:23] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    }
  ]
}