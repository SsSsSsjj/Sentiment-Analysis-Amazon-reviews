{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy nltk spacy xgboost gensim\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zBqRf8Jhxzcl",
        "outputId": "26fdf38c-f603-4e78-9d20-a9a7b26b5f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (6.2.2)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (642 bytes)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.5.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.12/dist-packages (from textacy) (3.6)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.0.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from textacy) (1.6.1)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.12/dist-packages (from textacy) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->textacy) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->textacy) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading floret-0.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, jellyfish, floret, cytoolz, gensim, textacy\n",
            "Successfully installed cytoolz-1.1.0 floret-0.10.5 gensim-4.4.0 jellyfish-1.2.1 pyphen-0.17.2 textacy-0.13.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYk1c7F-yb2L",
        "outputId": "653ccb31-f07a-4b96-a17f-90432e70700a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë ˆì´ë¸” ê¸°ì¤€ ì™„í™”: ìœ ìš©í•œ ë¦¬ë·°(HIGH) í‘œë³¸ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ helpful_percent ê¸°ì¤€ì„ 0.95ì—ì„œ 0.90ìœ¼ë¡œ ë‚®ì¶¤ (ë” ë§ì€ ë°ì´í„° í•™ìŠµ).\n",
        "\n",
        "Word2Vec í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: ì„ë² ë”© ë²¡í„° í¬ê¸°(vector_size)ë¥¼ 150ì—ì„œ 200ìœ¼ë¡œ ëŠ˜ë ¤ í‘œí˜„ë ¥ í–¥ìƒ.\n",
        "\n",
        "XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: HIGH í´ë˜ìŠ¤ì˜ ì¬í˜„ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ ë³µì¡ë„(max_depth)ë¥¼ 3ì—ì„œ 5ë¡œ ì¦ê°€ì‹œí‚¤ê³ , scale_pos_weightë¥¼ ì ìš©í•˜ì—¬ ê¸ì • í´ë˜ìŠ¤(HIGH)ì˜ ì¤‘ìš”ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZFOi_6AxzKrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì „ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ HIGH í´ë˜ìŠ¤ ì¬í˜„ìœ¨ì„ $100%$ë¡œ ìœ ì§€í•˜ë©´ì„œ LOW í´ë˜ìŠ¤ ì¬í˜„ìœ¨($62.5\\%$)ì„ ê°œì„ í•˜ê¸° ìœ„í•´ XGBoost ì˜ˆì¸¡ ì„ê³„ê°’ (Threshold) ì¡°ì • ë¡œì§ì„ ì¶”ê°€í•œ ìµœì¢… ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤.ì´ ì½”ë“œëŠ” í•™ìŠµëœ XGBoost ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼(í™•ë¥ )ë¥¼ ë°›ì•„, $0.5$ ëŒ€ì‹  ì‚¬ìš©ìê°€ ì§€ì •í•œ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ì„±ëŠ¥ ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "5zwgzq26zl-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gzip\n",
        "import spacy\n",
        "import pickle\n",
        "import string\n",
        "import ast\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.decomposition import NMF\n",
        "from nltk.corpus import stopwords\n",
        "import xgboost as XGB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(32113)\n",
        "\n",
        "##############################################################################\n",
        "#                      Stop Words and Lemmatizer setting                     #\n",
        "##############################################################################\n",
        "\n",
        "# STOPLIST ì •ì˜\n",
        "STOPLIST = list(\n",
        "    set(\n",
        "        stopwords.words(\"english\")\n",
        "        + [\"n't\", \"'s\", \"'m\", \"ca\"]\n",
        "        + list(ENGLISH_STOP_WORDS)\n",
        "    )\n",
        ") + \" \".join(string.punctuation).split(\" \") + [\n",
        "    \"-----\",\n",
        "    \"---\",\n",
        "    \"...\",\n",
        "    \"..\",\n",
        "    \"....\",\n",
        "    \"\",\n",
        "    \" \",\n",
        "    \"\\n\",\n",
        "    \"\\n\\n\",\n",
        "]\n",
        "\n",
        "# spaCy ëª¨ë¸ ë¡œë“œ\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    print(\"spaCy 'en_core_web_sm' ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "def tokenize_and_clean(doc):\n",
        "    \"\"\"\n",
        "    spaCy lemmaì™€ STOPLISTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™” ë° í´ë¦¬ë‹.\n",
        "    \"\"\"\n",
        "    if not isinstance(doc, str):\n",
        "        return []\n",
        "\n",
        "    # ìˆ«ì ì œê±°\n",
        "    doc = re.sub(r'\\d+', '', doc)\n",
        "\n",
        "    doc = nlp(doc)\n",
        "    tokens = []\n",
        "    for n in doc:\n",
        "        lemma = n.lemma_.lower().strip()\n",
        "        # ë¶ˆìš©ì–´, êµ¬ë‘ì , ì§§ì€ ë‹¨ì–´ ì œì™¸\n",
        "        if lemma not in STOPLIST and len(lemma) > 1 and n.pos_ != 'PUNCT':\n",
        "            token = n.lower_ if n.lemma_ == \"-PRON-\" else lemma\n",
        "            tokens.append(token)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#                             Data preparation                               #\n",
        "##############################################################################\n",
        "\n",
        "def Data_prep1(df):\n",
        "    \"\"\"\n",
        "    raw_reviews DataFrameì—ì„œ helpful, reviewText ê¸°ë°˜ìœ¼ë¡œ\n",
        "    helpful_percent, text_length ë“±ì„ ë§Œë“œëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    game_df = df.copy()\n",
        "\n",
        "    # 'helpful' ë¬¸ìì—´(\"[pos, total]\")ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "    game_df['helpful_parsed'] = game_df['helpful'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    helpful = np.array(game_df['helpful_parsed'])\n",
        "    helpful = helpful.reshape(len(game_df), 1)\n",
        "\n",
        "    helpful_num1 = np.zeros((len(helpful), 1)) # pos\n",
        "    helpful_num2 = np.zeros((len(helpful), 1)) # total\n",
        "\n",
        "    for i in range(len(helpful)):\n",
        "        try:\n",
        "            pos, total = helpful[i][0]\n",
        "        except (TypeError, ValueError, IndexError):\n",
        "            try:\n",
        "                pos, total = helpful[i]\n",
        "            except (TypeError, ValueError, IndexError):\n",
        "                 pos, total = 0, 0\n",
        "\n",
        "        helpful_num1[i] = pos\n",
        "        helpful_num2[i] = total\n",
        "\n",
        "    game_df['helpful_total_review'] = helpful_num2\n",
        "    game_df['num_of_helpful_review'] = helpful_num1\n",
        "\n",
        "    # ì´ íˆ¬í‘œ 20ê°œ ì´ìƒë§Œ ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œ ê¸°ì¤€ ìœ ì§€)\n",
        "    new_df = game_df[game_df['helpful_total_review'] > 20]\n",
        "\n",
        "    # ë¶ˆí•„ìš”í•œ ì—´ ì œê±°\n",
        "    drop_cols = [c for c in ['unixReviewTime', 'reviewTime', 'helpful_parsed', 'helpful']\n",
        "                 if c in new_df.columns]\n",
        "    new_df = new_df.drop(drop_cols, axis=1)\n",
        "\n",
        "    # helpful_percent ê³„ì‚°\n",
        "    new_df['helpful_percent'] = (\n",
        "        new_df['num_of_helpful_review'] / new_df['helpful_total_review']\n",
        "    ).round(2)\n",
        "    new_df.loc[new_df['helpful_total_review'] == 0, 'helpful_percent'] = 0\n",
        "\n",
        "\n",
        "    new_df.index = range(len(new_df))\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°\n",
        "    length = np.zeros((len(new_df), 1))\n",
        "    for i in new_df.index:\n",
        "        length[i] = int(len(str(new_df['reviewText'].iloc[i])))\n",
        "    new_df['text_length'] = length\n",
        "\n",
        "    # reviewTextê°€ NaNì¸ í–‰ ì œê±°\n",
        "    new_df = new_df.dropna(subset=['reviewText']).copy()\n",
        "\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def label_prep(new_df, line=0.90): # <- HIGH í´ë˜ìŠ¤ í™•ë³´ë¥¼ ìœ„í•´ 0.90ìœ¼ë¡œ ì™„í™”\n",
        "    \"\"\"\n",
        "    helpful_percentë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'label'('HIGH'/'LOW') ìƒì„±\n",
        "    \"\"\"\n",
        "    high = new_df[new_df['helpful_percent'] >= line]\n",
        "    low = new_df[new_df['helpful_percent'] < line]\n",
        "    print(f\"== line = {line} ==\")\n",
        "    print(\"highly helpful count: {}\".format(len(high)))\n",
        "    print(\"not helpful count: {}\".format(len(low)))\n",
        "\n",
        "    new_df['label'] = 'LOW'\n",
        "    new_df.loc[new_df.helpful_percent >= line, 'label'] = 'HIGH'\n",
        "    return new_df.copy()\n",
        "\n",
        "\n",
        "def create_w2v_features(df, vector_size=200, window=5, min_count=2):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1. í† í°í™” (ì „ì²˜ë¦¬)\n",
        "    print(\"Tokenizing reviews...\")\n",
        "    tokenized_docs = [tokenize_and_clean(text) for text in df['reviewText']]\n",
        "\n",
        "    # 2. Word2Vec ëª¨ë¸ í•™ìŠµ\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    w2v_model = Word2Vec(\n",
        "        tokenized_docs,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=4,\n",
        "        seed=32113\n",
        "    )\n",
        "\n",
        "    # 3. ë¬¸ì„œ ë²¡í„° ìƒì„± (Averaging Word Vectors)\n",
        "    print(\"Creating document vectors...\")\n",
        "\n",
        "    doc_vectors = []\n",
        "    for doc in tokenized_docs:\n",
        "        word_vectors = [w2v_model.wv[word] for word in doc if word in w2v_model.wv]\n",
        "\n",
        "        if word_vectors:\n",
        "            # ë‹¨ì–´ ë²¡í„°ì˜ í‰ê· ì„ ë¬¸ì„œ ë²¡í„°ë¡œ ì‚¬ìš©\n",
        "            doc_vector = np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            # ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ë§Œ ìˆê±°ë‚˜ ë¬¸ì„œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°, 0 ë²¡í„° ì‚¬ìš©\n",
        "            doc_vector = np.zeros(vector_size)\n",
        "\n",
        "        doc_vectors.append(doc_vector)\n",
        "\n",
        "    print(f\"Word2Vec feature creation took {time.time() - start_time:.2f} seconds.\")\n",
        "    return np.array(doc_vectors), w2v_model\n",
        "\n",
        "\n",
        "def _dataselector(df, label, limit):\n",
        "    \"\"\"\n",
        "    í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•´ ìµœëŒ€ 'limit' ê°œìˆ˜ë§Œí¼ ë°ì´í„° ì¶”ì¶œ\n",
        "    \"\"\"\n",
        "    df_lb_all = df[df['label'] == label]\n",
        "    if len(df_lb_all) > limit:\n",
        "        random_c = np.random.choice(list(df_lb_all.index), limit, replace=False)\n",
        "        df_lb = df_lb_all.loc[list(random_c)]\n",
        "    else:\n",
        "        df_lb = df_lb_all\n",
        "    return df_lb\n",
        "\n",
        "def df_for_XGBOOST(df_feat, lim):\n",
        "    \"\"\"\n",
        "    ë°ì´í„°í”„ë ˆì„ì˜ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ë§ì¶”ê³  XGBoost ì…ë ¥ìš© numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    df = df_feat.copy()\n",
        "    df.index = range(len(df))\n",
        "\n",
        "    df_low = _dataselector(df, 'LOW', lim)\n",
        "    df_high = _dataselector(df, 'HIGH', lim)\n",
        "\n",
        "    new_df = pd.concat([df_low, df_high], axis=0)\n",
        "    new_df.index = range(len(new_df)) # ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
        "\n",
        "    Y = new_df.pop('label') # <- 'label'ì„ ì¶”ì¶œí•˜ë©° new_dfì—ì„œ ì œê±°ë¨\n",
        "    b_loon = {'LOW': 0, 'HIGH': 1}\n",
        "    Y2 = Y.map(b_loon)\n",
        "\n",
        "    X = np.array(new_df)\n",
        "    return X, Y2, new_df # X: í”¼ì²˜, Y2: ë ˆì´ë¸”, new_df: í”¼ì²˜ DataFrame\n",
        "\n",
        "def XGBOOSTING(X_tr1, X_te1, y_tr1, y_te1, scale_pos_weight, xgb_para=[200, 0.2, 5]):\n",
        "    \"\"\"\n",
        "    XGBoost ëª¨ë¸ í•™ìŠµ ë° ì •í™•ë„ ì¶œë ¥\n",
        "    xgb_para: [n_estimators, learning_rate, max_depth]\n",
        "    scale_pos_weight: ê¸ì • í´ë˜ìŠ¤(HIGH)ì˜ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    xgb = XGB.XGBClassifier(\n",
        "        n_estimators=xgb_para[0],\n",
        "        learning_rate=xgb_para[1],\n",
        "        max_depth=xgb_para[2], # <- max_depthë¥¼ 5ë¡œ ì¦ê°€\n",
        "        eval_metric='logloss',\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=32113,\n",
        "        use_label_encoder=False,\n",
        "        scale_pos_weight=scale_pos_weight\n",
        "    )\n",
        "    xgb.fit(X_tr1, y_tr1)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    # ê¸°ë³¸ 0.5 ì„ê³„ê°’ì˜ ì •í™•ë„ ì¶œë ¥\n",
        "    score = xgb.score(X_te1, y_te1)\n",
        "    print(f\"XGBoost accuracy (Default 0.5 Threshold): {round(score * 100, 2)}%\")\n",
        "    return xgb\n",
        "\n",
        "def conf_mat(model, X_te1, y_te1, threshold=0.5): # <- ì„ê³„ê°’ íŒŒë¼ë¯¸í„° ì¶”ê°€\n",
        "    \"\"\"\n",
        "    Confusion matrix ë° í´ë˜ìŠ¤ë³„ ì¬í˜„ìœ¨ ì¶œë ¥\n",
        "    \"\"\"\n",
        "    # ì„ê³„ê°’ ì ìš©í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    y_proba = model.predict_proba(X_te1)[:, 1]\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_te1, y_pred)\n",
        "\n",
        "    print(f\"\\n** Confusion Matrix (Threshold: {threshold}) **\")\n",
        "    print(\"                 | Pred LOW (0) | Pred HIGH (1) |\")\n",
        "    print(\"-----------------|--------------|---------------|\")\n",
        "    print(f\"True LOW (0)     | {cm[0, 0]:<12} | {cm[0, 1]:<13} |\")\n",
        "    print(f\"True HIGH (1)    | {cm[1, 0]:<12} | {cm[1, 1]:<13} |\")\n",
        "\n",
        "    low_den = cm[0].sum()\n",
        "    high_den = cm[1].sum()\n",
        "\n",
        "    low_rate = (cm[0, 0] / low_den * 100).round(2) if low_den > 0 else 0.0\n",
        "    high_rate = (cm[1, 1] / high_den * 100).round(2) if high_den > 0 else 0.0\n",
        "\n",
        "    overall_accuracy = (cm[0, 0] + cm[1, 1]) / (low_den + high_den) * 100\n",
        "\n",
        "    print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n",
        "    print(f\"LOW (0) prediction rate (Recall): {low_rate}%\")\n",
        "    print(f\"HIGH (1) prediction rate (Recall): {high_rate}%\\n\")\n",
        "\n",
        "\n",
        "def xgb_stats(model, df_for_model, X_test, y_test, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Feature Importanceì™€ Confusion matrixë¥¼ ì¶œë ¥\n",
        "    \"\"\"\n",
        "    # 0. í”¼ì²˜ ì¤‘ìš”ë„\n",
        "    ind = np.argsort(model.feature_importances_)\n",
        "    imp = np.sort(model.feature_importances_)\n",
        "\n",
        "    imp2 = [df_for_model.columns[i] for i in ind]\n",
        "\n",
        "    print(\" **TOP 15 Important Features** \")\n",
        "    for i in range(1, 16):\n",
        "        print(\"{}. {} : {}%  \".format(i, imp2[-i], round(imp[-i] * 100, 2)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 1. Confusion Matrix ë° Classë³„ ì •í™•ë„ (ì„ê³„ê°’ 0.5 ê¸°ì¤€)\n",
        "    conf_mat(model, X_test, y_test, threshold=0.5)\n",
        "\n",
        "    # 2. Confusion Matrix ë° Classë³„ ì •í™•ë„ (íŠœë‹ëœ ì„ê³„ê°’ ê¸°ì¤€)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ¨ Tunning Result: Adjusted Threshold for better LOW Recall âœ¨\")\n",
        "    # LOW ì¬í˜„ìœ¨ ê°œì„ ì„ ìœ„í•´ ì„ê³„ê°’ì„ 0.5ë³´ë‹¤ ë†’ê²Œ ì„¤ì • (ì˜ˆ: 0.60)\n",
        "    # 0.60ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ HIGH ì˜ˆì¸¡ì„ ë” ì—„ê²©í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
        "    conf_mat(model, X_test, y_test, threshold=threshold)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#                                MAIN EXECUTION                              #\n",
        "##############################################################################\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ (ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ë¡œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
        "DATA_PATH = \"/content/drive/MyDrive/amazon_P/data/raw/Musical_instruments_reviews.csv\"\n",
        "try:\n",
        "    raw_reviews = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ì—ëŸ¬: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {DATA_PATH}\")\n",
        "    raise\n",
        "\n",
        "print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
        "\n",
        "# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë ˆì´ë¸”ë§ (HIGH/LOW ìƒì„±, ê¸°ì¤€ 0.90 ì ìš©)\n",
        "df_processed = Data_prep1(raw_reviews)\n",
        "df_labeled = label_prep(df_processed, line=0.90) # <- ê¸°ì¤€ 0.90 ì ìš©\n",
        "print(df_labeled['label'].value_counts())\n",
        "\n",
        "# 3. Word2Vec íŠ¹ì§• ì¶”ì¶œ (ë²¡í„° í¬ê¸° 200 ì ìš©)\n",
        "W2V_VECTOR_SIZE = 200\n",
        "X_w2v, w2v_model = create_w2v_features(df_labeled, vector_size=W2V_VECTOR_SIZE)\n",
        "\n",
        "# 4. í”¼ì²˜ ë³‘í•©\n",
        "df_w2v = pd.DataFrame(X_w2v, index=df_labeled.index,\n",
        "                      columns=[f'w2v_feat_{i}' for i in range(W2V_VECTOR_SIZE)])\n",
        "\n",
        "numerical_features = df_labeled[['overall', 'text_length']].copy()\n",
        "\n",
        "# ëª¨ë“  í”¼ì²˜ ë³‘í•© ë° ë ˆì´ë¸” ì¶”ê°€\n",
        "df_final_features = pd.concat([numerical_features, df_w2v, df_labeled[['label']]], axis=1)\n",
        "\n",
        "# 5. XGBoostìš© ë°ì´í„° ì¤€ë¹„ ë° í´ë˜ìŠ¤ ë°¸ëŸ°ì‹±\n",
        "class_counts = df_final_features['label'].value_counts()\n",
        "lim = int(class_counts.min())\n",
        "print(f\"\\nBalancing classes to limit={lim} per class.\")\n",
        "\n",
        "# X: features, y: label (0/1), df_for_model: feature DF (balanced)\n",
        "X, y, df_for_model = df_for_XGBOOST(df_final_features, lim=lim)\n",
        "\n",
        "# 6. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=32113, stratify=y\n",
        ")\n",
        "\n",
        "# 7. XGBoost ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì ìš©)\n",
        "scale_pos_weight = 1.0\n",
        "\n",
        "print(\"\\nStarting XGBoost Training with Tunning...\")\n",
        "xgb_w2v_model = XGBOOSTING(\n",
        "    X_tr,\n",
        "    X_te,\n",
        "    y_tr,\n",
        "    y_te,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    xgb_para=[200, 0.2, 5] # max_depth=5 ì ìš©\n",
        ")\n",
        "\n",
        "# 8. ê²°ê³¼ í†µê³„ ì¶œë ¥ ë° ì„ê³„ê°’ íŠœë‹\n",
        "# LOW ì¬í˜„ìœ¨ ê°œì„ ì„ ìœ„í•´ ì„ê³„ê°’ì„ 0.60ìœ¼ë¡œ ì„¤ì • (HIGH ì˜ˆì¸¡ ê¸°ì¤€ì„ ë†’ì„)\n",
        "ADJUSTED_THRESHOLD = 0.60\n",
        "\n",
        "feature_names_df = df_for_model.copy()\n",
        "xgb_stats(xgb_w2v_model, feature_names_df, X_te, y_te, threshold=ADJUSTED_THRESHOLD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az42-R6lx3n5",
        "outputId": "bdab7992-54cd-4dfa-8b4c-df7fce4814fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10261 reviews.\n",
            "== line = 0.9 ==\n",
            "highly helpful count: 119\n",
            "not helpful count: 40\n",
            "label\n",
            "HIGH    119\n",
            "LOW      40\n",
            "Name: count, dtype: int64\n",
            "Tokenizing reviews...\n",
            "Training Word2Vec model...\n",
            "Creating document vectors...\n",
            "Word2Vec feature creation took 14.75 seconds.\n",
            "\n",
            "Balancing classes to limit=40 per class.\n",
            "\n",
            "Starting XGBoost Training with Tunning...\n",
            "--- 0.12548494338989258 seconds ---\n",
            "XGBoost accuracy (Default 0.5 Threshold): 56.25%\n",
            " **TOP 15 Important Features** \n",
            "1. w2v_feat_194 : 10.470000267028809%  \n",
            "2. w2v_feat_195 : 6.130000114440918%  \n",
            "3. w2v_feat_84 : 5.809999942779541%  \n",
            "4. w2v_feat_47 : 4.550000190734863%  \n",
            "5. w2v_feat_133 : 4.369999885559082%  \n",
            "6. overall : 3.940000057220459%  \n",
            "7. w2v_feat_0 : 3.5999999046325684%  \n",
            "8. w2v_feat_11 : 3.5799999237060547%  \n",
            "9. w2v_feat_34 : 3.440000057220459%  \n",
            "10. w2v_feat_1 : 3.4200000762939453%  \n",
            "11. w2v_feat_19 : 3.390000104904175%  \n",
            "12. w2v_feat_153 : 3.369999885559082%  \n",
            "13. w2v_feat_56 : 3.319999933242798%  \n",
            "14. text_length : 3.140000104904175%  \n",
            "15. w2v_feat_68 : 2.5899999141693115%  \n",
            "\n",
            "\n",
            "\n",
            "** Confusion Matrix (Threshold: 0.5) **\n",
            "                 | Pred LOW (0) | Pred HIGH (1) |\n",
            "-----------------|--------------|---------------|\n",
            "True LOW (0)     | 4            | 4             |\n",
            "True HIGH (1)    | 3            | 5             |\n",
            "\n",
            "Overall Accuracy: 56.25%\n",
            "LOW (0) prediction rate (Recall): 50.0%\n",
            "HIGH (1) prediction rate (Recall): 62.5%\n",
            "\n",
            "\n",
            "==================================================\n",
            "âœ¨ Tunning Result: Adjusted Threshold for better LOW Recall âœ¨\n",
            "\n",
            "** Confusion Matrix (Threshold: 0.6) **\n",
            "                 | Pred LOW (0) | Pred HIGH (1) |\n",
            "-----------------|--------------|---------------|\n",
            "True LOW (0)     | 5            | 3             |\n",
            "True HIGH (1)    | 4            | 4             |\n",
            "\n",
            "Overall Accuracy: 56.25%\n",
            "LOW (0) prediction rate (Recall): 62.5%\n",
            "HIGH (1) prediction rate (Recall): 50.0%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [02:05:37] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ ê²°ê³¼ëŠ” ëª¨ë¸ì´ HIGHì™€ LOW í´ë˜ìŠ¤ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ íŠ¹ì§• íŒ¨í„´ì„ ì•„ì§ ì°¾ì§€ ëª»í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì„ê³„ê°’ ì¡°ì •ì€ í•œ ìª½ Recallì„ ê°œì„ í•˜ë©´ ë‹¤ë¥¸ ìª½ Recallì´ í•˜ë½í•˜ëŠ” Trade-offë§Œ ë°œìƒì‹œì¼°ìŠµë‹ˆë‹¤.ê·¼ë³¸ì ì¸ ì›ì¸:ë°ì´í„° ë¶€ì¡±: ì´ ë¦¬ë·° $10261$ê°œ ì¤‘ $20$ê°œ ì´ˆê³¼ íˆ¬í‘œë¥¼ í•œ ë¦¬ë·°ëŠ” $159$ê°œ, ìµœì¢… ë°¸ëŸ°ì‹±ëœ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” $80$ê°œ ($40$ê°œ/$40$ê°œ)ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. íŠ¹íˆ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ $16$ê°œë°–ì— ì•ˆ ë˜ë¯€ë¡œ, ëª¨ë¸ì´ ì•ˆì •ì ì¸ ì˜ˆì¸¡ íŒ¨í„´ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.Weak Feature (ì•½í•œ íŠ¹ì§•): Word2Vecì˜ ë‹¨ìˆœ í‰ê·  ë²¡í„°ê°€ ìœ ìš©ì„±($HIGH$/$LOW$)ì„ êµ¬ë¶„í•  ë§Œí¼ ê°•ë ¥í•œ íŠ¹ì§•ì„ ìƒì„±í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.ğŸ’¡ ìµœì¢… ì œì•ˆ: Doc2Vec ë˜ëŠ” ì•™ìƒë¸” ì ìš©Word2Vecì˜ ë‹¨ìˆœ í‰ê·  ëŒ€ì‹ , ë¬¸ì„œ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ëŠ” **Doc2Vec (Paragraph Vector)**ì„ ì‚¬ìš©í•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ í”¼ì²˜ + ìˆ˜ì¹˜ í”¼ì²˜ë¥¼ ê²°í•©í•œ ì•™ìƒë¸” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì˜ ì§ˆì„ ë†’ì´ëŠ” ê²ƒì´ ë‹¤ìŒ ë‹¨ê³„ì…ë‹ˆë‹¤.í˜„ì¬ ì½”ë“œì—ì„œ Doc2Vecì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ create_w2v_features í•¨ìˆ˜ë¥¼ ëŒ€ì²´í•˜ëŠ” ì½”ë“œë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "o103xHdcz_LW"
      }
    }
  ]
}